## **Word Embeddings**

### **1. Why is using one-hot encoding an inefficient towards vectorizing a corpus of words?  How are word embeddings different? (see this video https://www.youtube.com/watch?v=EEk6OiOOT2c )**

The use of one-hot encoding is inefficient towards vectoring a corpus of words as it involves the input of the number of words in rows and columns. Word embedding not only does the same as one-hot encoding but it can also get trained to understand the meanings of certain words through a model.

### **2. Compile and train the model from the tensorflow exercise.  Plot the training and validation loss as well as accuracy.  Post your plots and describe them.**

![image](https://user-images.githubusercontent.com/67992204/89113713-71c6b780-d442-11ea-9213-a2b2b414f6d8.png)

The above graph represents the training and validation loss over a certain number of epochs.

![image](https://user-images.githubusercontent.com/67992204/89113720-81460080-d442-11ea-8647-0a6a9ecce8c1.png)

### **3.Stretch Goal:  Follow the link to the Embedding Projector provided at the end of the exercise.  Produce the visualization of your embeddings.  Interpret your visualization.  What is it describing?  Is there relevance with regard to words that are proximate to each other?**

## Text Classification with an RNN

### **1. Again compile and train the model from the tensorflow exercise.  Plot the training and validation loss as well as accuracy.  Stack two or more LSTM layers in your model.  Post your plots and describe them**
